{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7902951,
          "sourceType": "datasetVersion",
          "datasetId": 4641655
        }
      ],
      "dockerImageVersionId": 30761,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebookf31a3738e3",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maks3174/Phy/blob/main/notebookf31a3738e3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'telegram-spam-or-ham:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4641655%2F7902951%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240827%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240827T163943Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D17e46f9960ec6240a1947cc886ebd761feb597b4faa89fe4d39f8bcf10ed978dff2bba9fa55c3a72d548651356170713c676d8d5c220298735a494f6d0ccb91fa1662722eeb4ca71224bd93064f2b57d10510c59a6696df40ca00aa2fbb312f27e8ef0faed54727439e24290fd9f0ec94f9e2e89d65a004f6580aa2212c2c64e126ec5c3fd8dfb5cc77a7dd52340aef15777f9a050b3c8fe065247e0da651b9b45429efed7f7fe2442d0af260a2bd96c90339f354da137b01818d888e12fd3d2d9e5be98507592ff0b4c88071657054c791fff0749826635662ff5444ff740fb13250bb3d9c0770803b1e7f1c2f8efd8af4cdd85b879d6fc34bef6162b7ad7c2'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "GTj0yG1ftTi1"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "\n",
        "folder = \"/kaggle/input/news-articles-classification-dataset-for-nlp-and-ml\"\n",
        "\n",
        "df1 = pd.read_csv(\"/kaggle/input/telegram-spam-or-ham/dataset.csv\")\n",
        "df2 = pd.read_csv(\"/kaggle/input/dataset/dataset.csv\")\n",
        "\n",
        "df = pd.concat([df1,df2])\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "5qEI5Zw2tTi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torchtext\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X, y, max_len=100):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.label_encoder = LabelEncoder().fit(y)\n",
        "        #self.vocab = torchtext.vocab.GloVe(name='6B', dim=50)\n",
        "        self.vocab = torchtext.vocab.Vectors(\"/kaggle/input/glove-6b-50d/glove.6B.50d.txt\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.label_encoder.transform([self.y.iloc[idx]])\n",
        "        label = torch.tensor(label)\n",
        "\n",
        "        text = self.X.iloc[idx]\n",
        "        tokens = text.split()\n",
        "\n",
        "        if len(tokens) > self.max_len:\n",
        "            tokens = tokens[:self.max_len]\n",
        "        else:\n",
        "            diff = self.max_len - len(tokens)\n",
        "\n",
        "            tokens += ['<pad>'] * diff\n",
        "\n",
        "        X = self.vocab.get_vecs_by_tokens(tokens, lower_case_backup=True)\n",
        "\n",
        "        return X, label[0]\n",
        "\n",
        "\n",
        "dataset = MyDataset(df['text_type'], df['text'])"
      ],
      "metadata": {
        "id": "dqQuY9RJtTi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torchtext\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X, y, max_len=100):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.label_encoder = LabelEncoder().fit(y)\n",
        "        #self.vocab = torchtext.vocab.GloVe(name='6B', dim=50)\n",
        "        self.vocab = torchtext.vocab.Vectors(\"/kaggle/input/glove-6b-50d/glove.6B.50d.txt\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.label_encoder.transform([self.y.iloc[idx]])\n",
        "        label = torch.tensor(label)\n",
        "\n",
        "        text = self.X.iloc[idx]\n",
        "        tokens = text.split()\n",
        "\n",
        "        if len(tokens) > self.max_len:\n",
        "            tokens = tokens[:self.max_len]\n",
        "        else:\n",
        "            diff = self.max_len - len(tokens)\n",
        "\n",
        "            tokens += ['<pad>'] * diff\n",
        "\n",
        "        X = self.vocab.get_vecs_by_tokens(tokens, lower_case_backup=True)\n",
        "\n",
        "        return X, label[0]\n",
        "\n",
        "\n",
        "dataset = MyDataset(df['text'], df['text_type'])"
      ],
      "metadata": {
        "id": "OxtyXuGptTi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0][0].shape"
      ],
      "metadata": {
        "id": "K-Ot1nkjtTi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_ratio = 0.8\n",
        "\n",
        "# Розділіть набір даних\n",
        "train_data, test_data = random_split(dataset, [train_ratio, 1-train_ratio])"
      ],
      "metadata": {
        "id": "N8RgqgwZtTi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "1W2Wjq79tTi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "# Код змінено для batch_first=True\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "NVbRoTZPtTi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, encoding_dim, max_len, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model=encoding_dim, max_len=max_len)\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=encoding_dim, nhead=2, batch_first=True, dim_feedforward=64),\n",
        "            num_layers=1\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.linear1 = nn.Linear(encoding_dim*max_len, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pos_encoder(x)\n",
        "        out = self.encoder(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.linear1(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def predict(self, X, device='cpu'):\n",
        "        X = torch.FloatTensor(np.array(X)).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_pred = F.softmax(self.forward(X), dim=-1)\n",
        "\n",
        "        return y_pred.cpu().numpy()\n",
        "\n",
        "\n",
        "model = TextClassifier(encoding_dim=50, max_len=100, num_classes=5).to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "zaAB9EH8tTi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_num = 0\n",
        "\n",
        "for param in model.parameters():\n",
        "    size = param.size()\n",
        "\n",
        "    num = 1\n",
        "    for item in size:\n",
        "        num *= item\n",
        "    params_num += num\n",
        "\n",
        "params_num"
      ],
      "metadata": {
        "id": "Z37k0bmmtTi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "FPs_d5GftTi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train(model, optimizer, loss_fn, train_dl, val_dl,\n",
        "          metrics=None, metrics_name=None, epochs=20, device='cpu', task='regression'):\n",
        "    '''\n",
        "    Runs training loop for classification problems. Returns Keras-style\n",
        "    per-epoch history of loss and accuracy over training and validation data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Neural network model\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        Search space optimizer (e.g. Adam)\n",
        "    loss_fn :\n",
        "        Loss function (e.g. nn.CrossEntropyLoss())\n",
        "    train_dl :\n",
        "        Iterable dataloader for training data.\n",
        "    val_dl :\n",
        "        Iterable dataloader for validation data.\n",
        "    metrics: list\n",
        "        List of sklearn metrics functions to be calculated\n",
        "    metrics_name: list\n",
        "        List of matrics names\n",
        "    epochs : int\n",
        "        Number of epochs to run\n",
        "    device : string\n",
        "        Specifies 'cuda' or 'cpu'\n",
        "    task : string\n",
        "        type of problem. It can be regression, binary or multiclass\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dictionary\n",
        "        Similar to Keras' fit(), the output dictionary contains per-epoch\n",
        "        history of training loss, training accuracy, validation loss, and\n",
        "        validation accuracy.\n",
        "    '''\n",
        "\n",
        "    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n",
        "          (type(model).__name__, type(optimizer).__name__,\n",
        "           optimizer.param_groups[0]['lr'], epochs, device))\n",
        "\n",
        "    metrics = metrics if metrics else []\n",
        "    metrics_name = metrics_name if metrics_name else [metric.__name__ for metric in metrics]\n",
        "\n",
        "    history = {} # Collects per-epoch loss and metrics like Keras' fit().\n",
        "    history['loss'] = []\n",
        "    history['val_loss'] = []\n",
        "    for name in metrics_name:\n",
        "        history[name] = []\n",
        "        history[f'val_{name}'] = []\n",
        "\n",
        "    start_time_train = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # --- TRAIN AND EVALUATE ON TRAINING SET -----------------------------\n",
        "        start_time_epoch = time.time()\n",
        "\n",
        "        model.train()\n",
        "        history_train = {name: 0 for name in ['loss']+metrics_name}\n",
        "\n",
        "        for batch in train_dl:\n",
        "            x    = batch[0].to(device)\n",
        "            y    = batch[1].to(device)\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            y_pred = y_pred.detach().cpu().numpy()\n",
        "            y = y.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "            history_train['loss'] += loss.item() * x.size(0)\n",
        "            for name, func in zip(metrics_name, metrics):\n",
        "              try:\n",
        "                  history_train[name] += func(y, y_pred) * x.size(0)\n",
        "              except:\n",
        "                  if task == 'binary': y_pred_ = y_pred.round()\n",
        "                  elif task == 'multiclass': y_pred_ = y_pred.argmax(axis=-1)\n",
        "                  history_train[name] += func(y, y_pred_) * x.size(0)\n",
        "\n",
        "        for name in history_train:\n",
        "            history_train[name] /= len(train_dl.dataset)\n",
        "\n",
        "\n",
        "        # --- EVALUATE ON VALIDATION SET -------------------------------------\n",
        "        model.eval()\n",
        "        history_val = {'val_' + name: 0 for name in metrics_name+['loss']}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dl:\n",
        "                x    = batch[0].to(device)\n",
        "                y    = batch[1].to(device)\n",
        "                y_pred = model(x)\n",
        "                loss = loss_fn(y_pred, y)\n",
        "\n",
        "                y_pred = y_pred.cpu().numpy()\n",
        "                y = y.cpu().numpy()\n",
        "\n",
        "                history_val['val_loss'] += loss.item() * x.size(0)\n",
        "                for name, func in zip(metrics_name, metrics):\n",
        "                    try:\n",
        "                        history_val['val_'+name] += func(y, y_pred) * x.size(0)\n",
        "                    except:\n",
        "                        if task == 'binary': y_pred_ = y_pred.round()\n",
        "                        elif task == 'multiclass': y_pred_ = y_pred.argmax(axis=-1)\n",
        "\n",
        "                        history_val['val_'+name] += func(y, y_pred_) * x.size(0)\n",
        "\n",
        "        for name in history_val:\n",
        "            history_val[name] /= len(val_dl.dataset)\n",
        "\n",
        "        # PRINTING RESULTS\n",
        "\n",
        "        end_time_epoch = time.time()\n",
        "\n",
        "        for name in history_train:\n",
        "            history[name].append(history_train[name])\n",
        "            history['val_'+name].append(history_val['val_'+name])\n",
        "\n",
        "        total_time_epoch = end_time_epoch - start_time_epoch\n",
        "\n",
        "        print(f'Epoch {epoch+1:4d} {total_time_epoch:4.0f}sec', end='\\t')\n",
        "        for name in history_train:\n",
        "            print(f'{name}: {history[name][-1]:10.3g}', end='\\t')\n",
        "            print(f\"val_{name}: {history['val_'+name][-1]:10.3g}\", end='\\t')\n",
        "        print()\n",
        "\n",
        "    # END OF TRAINING LOOP\n",
        "\n",
        "    end_time_train       = time.time()\n",
        "    total_time_train     = end_time_train - start_time_train\n",
        "    print()\n",
        "    print('Time total:     %5.2f sec' % (total_time_train))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "iSz0FBk6tTi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "history = train(model, optimizer, loss_fn, train_loader, test_loader,\n",
        "                epochs=10,\n",
        "                metrics=[accuracy_score],\n",
        "                device=device,\n",
        "                task='multiclass')"
      ],
      "metadata": {
        "id": "A6Mz7clqtTi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metric(history, name):\n",
        "    plt.title(f\"Model results with {name}\")\n",
        "    plt.plot(history[name], label='train')\n",
        "    plt.plot(history['val_'+name], label='val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "plot_metric(history, 'loss')"
      ],
      "metadata": {
        "id": "PChcNwx8tTi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(history, 'accuracy_score')"
      ],
      "metadata": {
        "id": "ay6jH6hYtTi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "model = model.to('cpu')  # відключаємо від gpu\n",
        "\n",
        "loader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data))\n",
        "X_test, y_test = next(iter(loader))\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred.argmax(-1), display_labels=dataset.label_encoder.classes_)\n",
        "plt.xticks(rotation=90)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "tDHc3EuUtTi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred.argmax(-1), target_names=dataset.label_encoder.classes_))"
      ],
      "metadata": {
        "id": "zchla9OptTi-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}